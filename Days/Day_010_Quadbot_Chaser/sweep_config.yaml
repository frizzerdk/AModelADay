program: train.py
method: bayes
metric:
  name: reward/success_reward.mean.mean
  goal: maximize

parameters:

  # Observation space parameters
  env.observation_max_wheel_steer_vel:
    distribution: normal
    mu: 5
    sigma: 0.5
  env.observation_max_wheel_drive_vel:
    distribution: normal
    mu: 100
    sigma: 10
  env.observation_max_body_linear_vel:
    distribution: normal
    mu: 20
    sigma: 2
  env.observation_max_body_angular_vel:
    distribution: normal
    mu: 3
    sigma: 0.3
  env.observation_max_body_position:
    distribution: normal
    mu: 10
    sigma: 1


  # Reward parameters
  env.reward_scale_action_penalty:
    distribution: normal
    mu: 1
    sigma: 0.1
  env.reward_scale_drive_action_penalty:
    distribution: normal
    mu: 0.001
    sigma: 0.0001
  env.reward_scale_steer_action_penalty:
    distribution: normal
    mu: 1
    sigma: 0.1
  env.reward_scale_wheel_misalignment:
    distribution: normal
    mu: 0.5
    sigma: 0.05
  env.reward_scale_position_penalty:
    distribution: normal
    mu: 0.0
    sigma: 0.01
  env.reward_scale_success_reward:
    distribution: normal
    mu: 50
    sigma: 5
  env.reward_scale_position_error_change:
    distribution: normal
    mu: 0.0
    sigma: 0.01
  env.reward_scale_distance_closed_to_target:
    distribution: normal
    mu: 0.0
    sigma: 0.01



  # Model parameters
  model.learning_rate:
    distribution: normal
    mu: 0.0003
    sigma: 0.00003
  model.n_epochs:
    values: [2, 3, 4, 5]
  model.gamma:
    values: [0.85, 0.9, 0.95, 0.99, 0.995, 0.999, 0.9995]
  model.gae_lambda:
    values: [0.85, 0.9, 0.95, 0.99, 0.995, 0.999, 0.9995]
  model.clip_range:
    distribution: normal
    mu: 0.2
    sigma: 0.02
  model.ent_coef:
    distribution: normal
    mu: 0.0001
    sigma: 0.00001
  model.vf_coef:
    distribution: normal
    mu: 0.5
    sigma: 0.05
  model.max_grad_norm:
    distribution: normal
    mu: 0.5
    sigma: 0.05
  model.log_std_init:
    distribution: normal
    mu: -0.6
    sigma: 0.1
