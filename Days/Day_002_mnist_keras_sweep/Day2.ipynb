{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Sweeping with wandb and keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of this notebook is to:\n",
    "* Run a sweep on the kaggle mnist dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 15:53:07.632334: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-06 15:53:09.564799: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "import keras\n",
    "import torch\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May  6 15:53:15 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070 ...    Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   39C    P5             24W /   40W |      11MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Mar_28_02:18:24_PDT_2024\n",
      "Cuda compilation tools, release 12.4, V12.4.131\n",
      "Build cuda_12.4.r12.4/compiler.34097967_0\n",
      "Num GPUs Available:  1\n",
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 15:53:15.710906: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-06 15:53:15.713024: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-06 15:53:15.713336: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "# check for gpu\n",
    "!nvidia-smi\n",
    "# check for cuda\n",
    "!nvcc --version\n",
    "# use gpu\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Download the dataset from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digit-recognizer.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Define the directory where you want to download the data\n",
    "data_dir = \"MyDataset/mnist/raw\"  # './' represents the current directory\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(data_dir):\n",
    "    # If not, create the directory\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Move to that directory\n",
    "os.chdir(data_dir)\n",
    "competition_name = \"digit-recognizer\"\n",
    "# Download the data\n",
    "os.system(\"kaggle competitions download -c \" + competition_name)\n",
    "\n",
    "# Unzip the data\n",
    "with zipfile.ZipFile(\"digit-recognizer.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\".\")\n",
    "\n",
    "os.chdir(\"../../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data And prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "train_df = pd.read_csv('MyDataset/mnist/raw/train.csv')\n",
    "test_df = pd.read_csv('MyDataset/mnist/raw/test.csv')\n",
    "\n",
    "# Split features and labels\n",
    "y_train = train_df[\"label\"]\n",
    "x_train = train_df.drop(labels = [\"label\"], axis = 1)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "x_train = x_train.values\n",
    "y_train = y_train.values\n",
    "\n",
    "# Test data\n",
    "x_test = test_df.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspet the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (37800, 28, 28, 1)\n",
      "y_train shape: (37800,)\n",
      "37800 train samples\n",
      "28000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Make Validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.1, random_state=2)\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "x_val = x_val.reshape(-1, 28, 28, 1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "def is_sweep():\n",
    "    \"\"\"\n",
    "    Check if the current run is part of a WandB sweep.\n",
    "    \"\"\"\n",
    "    if wandb.run is None:\n",
    "        return False\n",
    "    return wandb.run.sweep_id is not None\n",
    "def load_and_override_config(config_dir, config_name, manual_overrides={}):\n",
    "    \"\"\"\n",
    "    Load configuration with Hydra, manually override parameters, and integrate with WandB.\n",
    "\n",
    "    Args:\n",
    "    - config_dir (str): Directory path where configuration files are stored.\n",
    "    - config_name (str): Name of the configuration file to load without the extension.\n",
    "    - manual_overrides (dict): Dictionary of parameters to override manually.\n",
    "\n",
    "    Returns:\n",
    "    - OmegaConf.DictConfig: The final configuration object after all overrides.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize Hydra and load the base configuration\n",
    "    # hydra.initialize(config_path=config_dir)\n",
    "    # cfg = hydra.compose(config_name + \".yaml\")\n",
    "    cfg = OmegaConf.load(f\"{config_dir}/{config_name}.yaml\")\n",
    "    \n",
    "    # Apply manual overrides\n",
    "    cfg = OmegaConf.merge(cfg, OmegaConf.create(manual_overrides))\n",
    "    \n",
    "    \n",
    "    # Check if running under WandB and apply WandB configuration if it's a sweep\n",
    "    if wandb.run is not None:\n",
    "        # Assuming wandb has been initialized outside this function in your main workflow\n",
    "        wandb_config = wandb.config\n",
    "        print(\"wandb_config\",wandb_config)\n",
    "        cfg = OmegaConf.merge(cfg, OmegaConf.create(dict(wandb_config)))\n",
    "\n",
    "    cfg.is_sweep= is_sweep()\n",
    "    print(\"cfg\",cfg)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "import wandb\n",
    "\n",
    "def get_or_create_sweep_id(project_name, sweep_config,force_create=False):\n",
    "    \"\"\"\n",
    "    Get or create a sweep ID for the given project.\n",
    "\n",
    "    This function checks if there is a file named '{project_name}_sweep_id.txt' that contains the sweep ID.\n",
    "    If the file exists, it reads the sweep ID from the file.\n",
    "    If the file does not exist, it creates a new sweep and writes the sweep ID to the file.\n",
    "\n",
    "    Args:\n",
    "    project_name (str): The name of the project.\n",
    "    sweep_config (dict): The configuration of the sweep.\n",
    "\n",
    "    Returns:\n",
    "    str: The sweep ID.\n",
    "    \"\"\"\n",
    "    sweep_id_folder = 'sweep_ids'\n",
    "    sweep_id_file = f'{project_name}_sweep_id.txt'\n",
    "    sweep_id_file = os.path.join(sweep_id_folder, sweep_id_file)\n",
    "    if force_create:\n",
    "        sweep_id = wandb.sweep(sweep_config, project=project_name)\n",
    "        with open(sweep_id_file, 'w') as file:\n",
    "            file.write(sweep_id)\n",
    "        return sweep_id\n",
    "    # Check if the sweep ID file exists\n",
    "    if os.path.exists(sweep_id_file):\n",
    "        # If the file exists, read the sweep ID from the file\n",
    "        with open(sweep_id_file, 'r') as file:\n",
    "            sweep_id = file.read().strip()\n",
    "    else:\n",
    "        # If the file does not exist, create a new sweep\n",
    "        sweep_id = wandb.sweep(sweep_config, project=project_name)\n",
    "        # Make sure the directory exists\n",
    "        os.makedirs(sweep_id_folder, exist_ok=True)\n",
    "        # Write the sweep ID to the file\n",
    "        with open(sweep_id_file, 'w') as file:\n",
    "            file.write(sweep_id)\n",
    "    \n",
    "    return sweep_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Manual Overrides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_overrides = {\n",
    "   # 'epochs': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 113549), started 17:40:46 ago. (Use '!kill 113549' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2170a097961cb5a5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2170a097961cb5a5\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg {'project_name': 'Day2', 'username': 'frizzerdk', 'epochs': 50, 'param_scale': 1.0, 'dropout_rate': 0.5, 'learning_rate': 0.001, 'batch_size': 128, 'patience': 20, 'is_sweep': False, 'checkpoint_path': './checkpoints', 'best_model_path': '${checkpoint_path}/best_model.keras'}\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir=logs\n",
    "from wandb.keras import WandbMetricsLogger, WandbEvalCallback\n",
    "from IPython.display import clear_output\n",
    "import gc\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0\n",
    "cfg = load_and_override_config(\".\", \"config\")\n",
    "def main():\n",
    "    # Load configuration\n",
    "    cfg = load_and_override_config(\".\", \"config\")\n",
    "    wandb.init(project=cfg.project_name)\n",
    "    print(OmegaConf.to_yaml(cfg))\n",
    "    cfg = load_and_override_config(\".\", \"config\")\n",
    "\n",
    "    wandb.config = OmegaConf.to_container(\n",
    "        cfg, resolve=True, throw_on_missing=True\n",
    "    )\n",
    "\n",
    "    num_classes = 10\n",
    "    input_shape = (28, 28, 1)\n",
    "\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Input(shape=input_shape),\n",
    "            keras.layers.Conv2D(int(64*cfg.param_scale), kernel_size=(3, 3), activation=\"relu\"),\n",
    "            keras.layers.Conv2D(int(64*cfg.param_scale), kernel_size=(3, 3), activation=\"relu\"),\n",
    "            keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            keras.layers.Conv2D(int(128*cfg.param_scale), kernel_size=(3, 3), activation=\"relu\"),\n",
    "            keras.layers.Conv2D(int(128*cfg.param_scale), kernel_size=(3, 3), activation=\"relu\"),\n",
    "            keras.layers.GlobalAveragePooling2D(),\n",
    "            keras.layers.Dropout(cfg.dropout_rate),\n",
    "            keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=cfg.learning_rate),\n",
    "    metrics=[\n",
    "        keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "    ],\n",
    "    )\n",
    "\n",
    "    callbacks = [ \n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_acc\",patience=cfg.patience, verbose=1, restore_best_weights=True),\n",
    "    keras.callbacks.ModelCheckpoint(cfg.best_model_path, save_best_only=True),\n",
    "    keras.callbacks.TensorBoard(log_dir=\"./logs\"), \n",
    "    WandbMetricsLogger(),\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=cfg.batch_size,\n",
    "        epochs=cfg.epochs,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=(x_val, y_val),\n",
    "\n",
    "    )\n",
    "\n",
    "        # Log the best model as an artifact\n",
    "    artifact = wandb.Artifact('best-model', type='model')\n",
    "    artifact.add_file(cfg.best_model_path)\n",
    "    wandb.log_artifact(artifact)\n",
    "    wandb.finish()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_sweep = True\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'metric': {'name': 'loss', 'goal': 'minimize'},\n",
    "    'parameters': {\n",
    "        'learning_rate': {'values': [ 0.001, 3e-4, 0.0001]},\n",
    "        'batch_size': {'values': [16, 32, 64]},\n",
    "        'param_scale': {'values': [0.5, 1, 2]},\n",
    "        'dropout_rate': {'values': [0.0,0.1, 0.5, 0.9]},\n",
    "    }\n",
    "}\n",
    "sweep_id = get_or_create_sweep_id(cfg.project_name, sweep_config)\n",
    "\n",
    "if do_sweep:\n",
    "    wandb.agent(sweep_id, project=cfg.project_name, function=main)\n",
    "else:\n",
    "    main()\n",
    "\n",
    "#wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 runs in the sweep.\n",
      "Identified the top 3 runs.\n",
      "Best overall run: unique-sweep-27\n",
      "Top run IDs: {'958pli3r', '2zr4i4om', 'vpwipunx'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ./best_models/unique-sweep-27.keras locally.\n",
      "Saved the best overall model as ./best_models/overall_best_model.keras.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ./best_models/silver-sweep-15.keras locally.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ./best_models/cerulean-sweep-42.keras locally.\n",
      "Could not process artifact for run dulcet-sweep-60: No artifact named best-model found for run dulcet-sweep-60\n",
      "Completed processing the models.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "import os\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "def cleanup_and_save_top_models(project_name, username, sweep_id, top_x, sort_metric=\"epoch/val_acc\", artifact_name=\"best-model\", delete_other=False, local_save_path=\"./best_models\"):\n",
    "    \"\"\"\n",
    "    Identifies the top X best runs from a Weights & Biases sweep,\n",
    "    deletes artifacts from the other runs, saves the top models locally for evaluation, and saves the best model overall as `overall_best_model`.\n",
    "\n",
    "    Args:\n",
    "        project_name (str): The name of the wandb project.\n",
    "        username (str): Your wandb username.\n",
    "        sweep_id (str): The sweep ID containing the runs.\n",
    "        top_x (int): The number of best runs to retain.\n",
    "        sort_metric (str): The metric name to use for sorting the best runs.\n",
    "        artifact_name (str): The name of the model artifact to save or delete.\n",
    "        delete_other (bool): Whether to delete artifacts that aren't in the top X.\n",
    "        local_save_path (str): Path where the top models will be saved locally.\n",
    "    \"\"\"\n",
    "    # Initialize the wandb API\n",
    "    api = wandb.Api()\n",
    "\n",
    "    # Construct the project path\n",
    "    project_path = f\"{username}/{project_name}\"\n",
    "\n",
    "    # Fetch all runs associated with the specified project and sweep\n",
    "    runs = api.runs(path=project_path, filters={\"sweep\": sweep_id})\n",
    "\n",
    "    # Sort runs by the specified metric, defaulting to 0 if the metric isn't found\n",
    "    sorted_runs = sorted(runs, key=lambda run: run.summary.get(sort_metric, 0), reverse=True)\n",
    "    print(f\"Found {len(sorted_runs)} runs in the sweep.\")\n",
    "\n",
    "    # Identify the top X runs\n",
    "    top_runs = sorted_runs[:top_x]\n",
    "    print(f\"Identified the top {top_x} runs.\")\n",
    "\n",
    "    # Get the best overall run\n",
    "    best_overall_run = sorted_runs[0]\n",
    "    print(f\"Best overall run: {best_overall_run.name}\")\n",
    "\n",
    "    # Create a set of run IDs to keep\n",
    "    top_run_ids = {run.id for run in top_runs}\n",
    "    print(f\"Top run IDs: {top_run_ids}\")\n",
    "\n",
    "    # Create a directory to save the top models locally\n",
    "    os.makedirs(local_save_path, exist_ok=True)\n",
    "\n",
    "    # Process each run and decide whether to save or delete its artifact\n",
    "    for run in sorted_runs:\n",
    "        try:\n",
    "            # Find the list of artifacts associated with the current run\n",
    "            artifacts = list(run.logged_artifacts())\n",
    "\n",
    "            # Find the artifact that matches the specified artifact_name\n",
    "            artifact = next((a for a in artifacts if artifact_name in a.name), None)\n",
    "\n",
    "            if artifact is None:\n",
    "                raise ValueError(f\"No artifact named {artifact_name} found for run {run.name}\")\n",
    "\n",
    "            if run.id in top_run_ids:\n",
    "                # Download and save the model locally if it's in the top X\n",
    "                artifact_dir = artifact.download()\n",
    "                local_model_path = os.path.join(local_save_path, f\"{run.name}.keras\")\n",
    "                os.rename(os.path.join(artifact_dir, \"best_model.keras\"), local_model_path)\n",
    "                print(f\"Saved {local_model_path} locally.\")\n",
    "\n",
    "                # Save the overall best model as `overall_best_model.keras`\n",
    "                if run == best_overall_run:\n",
    "                    overall_best_path = os.path.join(local_save_path, \"overall_best_model.keras\")\n",
    "                    os.rename(local_model_path, overall_best_path)\n",
    "                    print(f\"Saved the best overall model as {overall_best_path}.\")\n",
    "            else:\n",
    "                # Delete the artifact if it's not in the top X\n",
    "                if delete_other:\n",
    "                    artifact.delete()\n",
    "                    print(f\"Deleted artifact from run {run.name}.\")\n",
    "                else:\n",
    "                    print(f\"Skipping deletion of artifact from run {run.name}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process artifact for run {run.name}: {e}\")\n",
    "\n",
    "    print(\"Completed processing the models.\")\n",
    "\n",
    "# Example usage:\n",
    "# Ensure your `cfg` object has the project name and username details\n",
    "cleanup_and_save_top_models(cfg.project_name, cfg.username, sweep_id=sweep_id, top_x=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# Assuming 'predictions' is an array containing your model's predictions\n",
    "# load model\n",
    "best_model = keras.models.load_model(\"./best_models/overall_best_model.keras\")\n",
    "if best_model is not None:\n",
    "    best_model.summary()\n",
    "    score = best_model.evaluate(x_val, y_val)\n",
    "    print(\"Validation loss:\", score[0], \"Validation accuracy:\", score[1])\n",
    "    predictions = best_model.predict(x_test)\n",
    "    class_predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if predictions is not None:\n",
    "    \n",
    "    # Create a DataFrame with the prediction results\n",
    "    # 'ImageId' is a common column name in MNIST-like competitions\n",
    "    submission = pd.DataFrame({\n",
    "        \"ImageId\": list(range(1, len(class_predictions) + 1)),\n",
    "        \"Label\": class_predictions\n",
    "    })\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    submission.to_csv('my_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle competitions submit -c digit-recognizer -f my_submission.csv -m \"First submission\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submissions digit-recognizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
